# -*- coding: utf-8 -*-
# Created Time: 2017-07-19 13:47:52
import requests
import datetime
import json
from lxml import etree
from multiprocessing import Pool
from mongo_client import mongo
from concurrent import futures
import threading
L = threading.Lock()


def get_json(url):
    token_pool = [
            'Token token="6sK38KFc2GoG8n1hCh-wj3TN3JmdnFbtRJ3VRJuWTOH2imukhEaTXfuq0s13"',
            'Token token="BE5TAExeBFgiNLG6Z-fCntYH0QW-w06Z_mVb6HL_USXqFPuPINdWzpB9AFvh"',
            # 'Token token="EfA_jC7GQNnxYgmzT5ATEfdvL6CgF9g8n_7PtX9uORv6J6hGTm2jDiyGBRCu"',
            'Token token="eGDaUf0YfwrW5CqkKtzNshxbgPa3snJqS3pEGWscNgLytcW6kkvHRgM-ziWX"',
            'Token token="wdWVL9otOkHjGtC7uWp6ECQZnLanjiJIRAaRbp_0DnkAqFtUiun474re_hzW"',
            'Token token="HMC13NM3F4zZ--Cftx7r-cCLlblSOzuUWT4o5VNc4bqiAdwWADv1FwbMgrtz"',
            'Token token="xh4YNCY6pQYWJ6aF4RcBzb8GvgsuL8HjY-j7LjyQywfafwsO5Ebej4co93le"',
    ]

    headers = {
            'Accept': '*/*',
            'Accept-Language': 'en-CN',
            'Host': 'rutilus.fishbrain.com',
            'If-None-Match': 'W/"8a530dba8a3e222bf2046d9f25fd338d"',
            'Authorization': 'Token token="9ZLCd53-wBpt9lZ1e6LQKCU7PAtb6p992onudGgKXDRquOr0Z4lVRzuZh-b2"',
            'User-Agent': 'Fishbrain/6.0 (iPhone; iOS 12.0; Scale/2.00)',
            'Connection': 'keep-alive'}

    second = datetime.datetime.now().second
    token_length = len(token_pool)
    avg_second = 60 // token_length
    # 当前秒数除以 平均 秒数 = 第几个token
    # token_index = int(round(float(second) / float(avg_second)))
    token_index = second // avg_second
    headers['Authorization'] = token_pool[token_index]
    # cursor = mongo.find_one({"url": url}, "response")
    # if url in mongo_user:
        # return {}

    # if not cursor:
    for token in token_pool: 
        respone = requests.get(url, timeout=10, headers=headers)
        print url, respone.status_code, headers["Authorization"]
        if respone.status_code == 200:
            content = json.loads(respone.content)
            # 请求过的数据不再请求
            # mongo.upsert({"url": url}, {"content": content, "url": url}, "response")
            return content
        elif respone.status_code == 404:
            mongo.upsert({"url": url}, {"content": {}, "url": url}, "response")
            return {}
        else:
            headers['Authorization'] = token

    return {}
    # else:
        # return cursor.get("content", {})

def get_img(image_info, index=None):
    result = []
    if not image_info:return ''
    if image_info.get("sizes"):
        result = [info.get("url") for info in image_info.get("sizes")]
    if not result:
        return ''

    if index:
        try:
            return result[index]
        except:
            return result[0]
    else:
        return result

def get_following_from_user(user_info):
    """ 获取关注用户的人
    """
    max_length = 100
    user_id = user_info.get("user_id")
    url = "https://rutilus.fishbrain.com/users/{user_id}/followings/fishing_waters?page={page}&per_page={max_length}&verbosity=verbose"
    counts = user_info.get("followings_counters")
    if not counts: return []
    pages = counts // max_length

    if pages > 1:
        pages += 2
    else:
        pages = 2

    followings_user_id_list = []
    for i in range(1, pages):
        _url = url.format(user_id=user_id, page=i, max_length=max_length)
        content = get_json(_url)
        if not content:
            break
        page_list = [c.get("id") for c in content]
        followings_user_id_list.extend(page_list)
    return followings_user_id_list


def get_followers_from_user(user_info):
    """ 获取用户关注的人
    """
    counts = user_info.get("catches_count")
    if not counts: return []
    max_length = 100
    user_id = user_info.get("user_id")
    url = "https://rutilus.fishbrain.com/users/{user_id}/followers?page={page}&per_page={max_length}"
    if not counts: return
    pages = counts // max_length

    if pages > 1:
        pages += 2
    else:
        pages = 2

    followers_user_id_list = []
    for i in range(1, pages):
        _url = url.format(user_id=user_id, page=i, max_length=max_length)
        content = get_json(_url)
        if not content:
            break
        page_list = [c.get("id") for c in content] 
        followers_user_id_list.extend(page_list)

    return followers_user_id_list


def get_all_user(user_id):
    """ 获取所有用户数据
    """
    all_user_follow_cousor = mongo.find_one({"user_id": user_id}, "fishibrain_all_user")
    if all_user_follow_cousor:
        return all_user_follow_cousor.get("user_list")
    user_info = get_user_info(user_id)
    if not user_info:
        return []

    mongo.upsert({"user_id": user_id}, user_info, "users")
    print(user_id,"++++++++++++++++++++++++++++++++++++new+++++++++++++++++++++++++++++++")
    followers_user_id_list = get_followers_from_user(user_info) 
    followings_user_id_list = get_following_from_user(user_info)
    lst = followings_user_id_list + followers_user_id_list
    mongo.upsert({"user_id": user_id}, {"user_list": lst, "user_id": user_id}, "fishibrain_all_user")
    return lst

# 指定一个层数
def main_1(user_id, n=2):
    _all_user_id = get_all_user(user_id)
    digui(_all_user_id)

# TODO 存数据库
def digui(user_list, n=6):
    if n < 0:
        return
    print '++++++++++++++++++++++++++++++++++++++++++++++++++',n,'+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++'
    all_list = set()
    for uid in user_list:
        get_all_user(uid)

    all_user_follow_cousor = mongo.find_all({"user_id": {"in": user_list}}, "fishibrain_all_user")
    for user in all_user_follow_cousor:
        all_list.update(user.get("user_list"))
    # 维护所有没有user_id 的表
    digui(list(all_list), n-1)


# def main(user_id):
    # 获取用户的所有关注者和被关注者
    # all_user_id = get_all_user(user_id)
    # print user_id
    # for uid in all_user_id:
        # if mongo.find_one({"user_id": user_id}, "fishibrain_all_user"):
            # continue
        # if user_id not in all_user_id:
            # main(uid)
    # mongo.upsert({"user_id": user_id}, {"user_list": all_user_id, "user_id": user_id}, "fishibrain_all_user")



 
def get_user_info(user_id):
    """ 获取用户关注数 和 被关注数 鱼获总数
    """
    url = "https://rutilus.fishbrain.com/users/%s?expand=profile" % user_id
    content = get_json(url)
    if not content:
        return
    try:
        nickname = content.get("nickname")
        firstname = content.get("first_name")
        lastname = content.get("last_name")
        user_id = content.get("id")
        followers_count = content.get("followers_count")
        followings_counters = content.get("followings_counters").get("users")
        catches_count = content.get("catches_count")
        country_code = content.get("country_code")
        avatar_img = get_img(content.get("avatar"))

        user_info = {"user_id": user_id,
                     "nickname": nickname,
                     "firstname": firstname,
                     "lastname": lastname,
                     "followings_counters": followings_counters,
                     "followers_count": followers_count,
                     "country_code": country_code,
                     "avatar_img": avatar_img,
                     "catches_count": catches_count}
        return user_info
    except:
        return

def main(uid_list):
    for uid in uid_list:
        user_info = get_user_info(uid)
        print user_info
        if not user_info:
            continue
        else:
            c = mongo.upsert({"user_id": uid}, user_info, "users")
            print(c)



def muilt_thread(args):
    # args 为 list
    # executor.__exit__方法会调用executor.shutdown(wait=True)方法，
    # 它会在所有线程都执行完毕前阻塞线程

    avg = len(args) // 40
    args_list = (args[avg*i: avg+avg*i] for i in range(40))

    with futures.ThreadPoolExecutor(40) as executor:  # <5>
        # map 与内置map方法类似，不过download_one 函数会在多个线程中并发调用；
        # map 方法返回一个生成器，因此可以迭代，
        # 迭代器的__next__方法调用各个Future 的 result 方法
        res = executor.map(main, args_list)

    print(len(list(res)))


if __name__ == "__main__":
    # import multiprocessing
    # pool = multiprocessing.Pool(processes=4)
    # url = "https://rutilus.fishbrain.com/users/1000364/followers?page=1&per_page=40"
    # url = "https://rutilus.fishbrain.com/users/1977477/followers?page=1&per_page=100"

    mongo_user = mongo.find_all({"user_id": {"$ne": ""}}, "users")
    all_users = set([i.get("user_id") for i in mongo_user])
    print(len(all_users))
    user_list = set(range(3100000, 3200000)) ^ all_users
    print(len(user_list))

    muilt_thread(list(user_list))
    # total = len(user_list)
    # avg = total // 4
    # 将数据分为 4份 来处理
    # sub_lst = (user_list[avg*i: avg+avg*i] for i in range(4))
    # for lst in sub_lst:
    # pool.apply_async(muilt_thread, (lst, ))

    # pool.close()
    # pool.join()
